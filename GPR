import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, RationalQuadratic, ExpSineSquared, WhiteKernel, ConstantKernel, DotProduct, Matern
from sklearn import  preprocessing
from sklearn.metrics import mean_squared_error, r2_score
#准备数据


# load dataset
dataset = pd.read_csv('total_displacement.csv', header=0, index_col=0, sep=',')
values = dataset.values
ymax = values[:, 0].max()
ymin = values[:, 0].min()


scaler = preprocessing.MinMaxScaler()
# values = scaler.fit_transform(values)

#划分Training set and Testing set
X = values[:, 1:2]
print(X)

y = values[:, 0]
print(y)



sepIndex = 55

train_X, train_y = X[:sepIndex, :], y[:sepIndex]
test_X, test_y = X[sepIndex:, :], y[sepIndex:]




a = np.arange(len(train_y), len(train_y)+len(test_y), 1)
b = np.arange(0, len(train_y), 1)

#
# kernel = ConstantKernel(1.0, (1e-4, 1e4)) **2* RBF(length_scale=2.5, length_scale_bounds=(1e-4, 1e4)) +\
#          ConstantKernel(1.0, (1e-4, 1e4)) **2* RBF(length_scale=1.5, length_scale_bounds=(1e-4, 1e4)) +\
#          WhiteKernel(noise_level=10.0, noise_level_bounds=(1e-10, 1e+3)) +\
#          ConstantKernel(15.0, (1e-4, 1e4)) **2*RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-4, 1e4)) +\
#          ConstantKernel(10.0, (1e-4, 1e4)) **2*RationalQuadratic(length_scale=2.0, length_scale_bounds=(1e-4, 1e4)) +\
#          ConstantKernel(3.0, (1e-4, 1e4)) **2*RationalQuadratic(length_scale=2.0, length_scale_bounds=(1e-4, 1e4)) +\
#          ConstantKernel(10.0, (1e-4, 1e4)) ** 2* ExpSineSquared(length_scale=1.3, periodicity=1.0) +\
#          ConstantKernel(5.0, (1e-4, 1e4)) ** 2* ExpSineSquared(length_scale=1.3, periodicity=1.0) +\
#          ConstantKernel(0.2, (1e-4, 1e4))**2*(DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-4, 1e4)) ** 2)


kernel = ConstantKernel(1.0, (1e-4, 1e4)) * RBF(length_scale=1.0, length_scale_bounds=(1e-4, 1e4)) +\
         ConstantKernel(1.0, (1e-4, 1e4))*(DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-4, 1e4))) +\
         ConstantKernel(1.0, (1e-4, 1e4))*RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-4, 1e4)) +\
         ConstantKernel(1.0, (1e-4, 1e4))* ExpSineSquared(length_scale=1.0, periodicity=1.0)





gp = GaussianProcessRegressor(kernel=kernel, alpha=4000.0, n_restarts_optimizer=10)
gp.fit(train_X, train_y)
y_pred, y_std = gp.predict(test_X, return_std=True)
y_train_pred, y_train_std = gp.predict(train_X, return_std=True)
aa = pd.DataFrame(y_train_pred)
aa.to_excel('ZG110CPRED.xlsx')
bb = pd.DataFrame(y_pred)
bb.to_excel('ZG110TESTPRED.xlsx')
cc = pd.DataFrame(y_std)
cc.to_excel('y_pred_std.xlsx')
dd = pd.DataFrame(y_train_std)
dd.to_excel('y_train_pred_std.xlsx')
# y_train_pred = y_train_pred*(ymax-ymin) + ymin
# # y_trian = train_y*(ymax-ymin) + ymin
# # y_pred = y_pred*(ymax-ymin) + ymin
# # y_train_std = y_train_std*(ymax-ymin)
# # test_y = test_y*(ymax-ymin) + ymin
# # y_std = y_std*(ymax-ymin)
#
print('RMSE:', np.sqrt(mean_squared_error(y_pred, test_y)))
R = r2_score(y_pred, test_y)

print('R2: %.3f' % R)
#
print(y_pred)
print(test_y)
print(y_std)
print("\nLearned kernel: %s" % gp.kernel_)
print("Log-marginal-likelihood: %.3f"
      % gp.log_marginal_likelihood(gp.kernel_.theta))

plt.plot(b, y_train_pred, marker = 'o', markersize=3.0)
plt.plot(b, train_y, marker = 's', markersize =2.5)
plt.fill_between(b, y_train_pred-1.96*y_train_std, y_train_pred+1.96*y_train_std, alpha = 0.5, color = 'gray')
plt.plot(a, test_y, marker = 'o',markersize =2.5)
plt.plot(a, y_pred, marker = 's',markersize =3.0)
plt.fill_between(a, y_pred-1.96*y_std, y_pred+1.96*y_std, alpha = 0.5, color = 'gray')

plt.show()



# #概率密度预测图
# import math
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
#
# u = 997   # 均值μ
# sig = 10.7  # 标准差δ
# x0 = 1006
# y0 = 0.04
# x = np.linspace(u - 5*sig, u + 5*sig, 100)
#
# y = np.exp(-(x - u)**2/(2* sig **2))/(math.sqrt(2*math.pi)*sig)
#
# plt.plot(x, y, "r-", linewidth=2)
# plt.bar(x0, y0, width=0.7, color = 'blue')
# plt.grid(True)
# plt.show()

#概率密度评价CRPS 与PIT
from scipy import stats
from statsmodels.graphics.api import qqplot
plt.rc('font', family='Times New Roman')
from scipy.stats import norm
def getCRPS(predictions, sigmas, observations):
    # only for gaussian distribution
    num = len(observations)
    areas = np.zeros(shape=(num, 1))
    for i in range(num):
        x1 = norm.ppf(0.9999, loc=predictions[i], scale=sigmas[i])
        x0 = norm.ppf(0.0001, loc=predictions[i], scale=sigmas[i])
        print(x1)
        print(x0)
        if x1 < observations[i]:
            x1 = observations[i]

        if x0 > observations[i]:
            x0 = observations[i]

        x = np.linspace(x0, x1, 1000)
        y = np.zeros(shape=x.shape)
        area = 0.0
        for j in range(len(x)):
            y[j] = norm.cdf(x[j], loc=predictions[i], scale=sigmas[i]) - getH(x[j], observations[i])
            y[j] = np.power(y[j], 2)
            if j >= 1:
                area = area + (y[j] + y[j - 1]) * (x[j] - x[j - 1]) / 2
        areas[i, 0] = area
    crps = np.mean(areas)
    return crps

def getH(prediction, observation):
    if prediction < observation:
        return 0
    else:
        return 1


def getPIT(predictions, sigmas, observations,):
    PIT = np.zeros(shape=observations.shape)
    for i in range(observations.shape[0]):
        PIT[i] = norm.cdf(observations[i], loc=predictions[i], scale=sigmas[i])
    return PIT


def getReliabilityMetric(predictions, sigmas, observations, metricNames=None):
    if metricNames is None:
        metricNames = ['PIT']
    metrics = {}
    for metricName in metricNames:
        metric = None
        if metricName == 'PIT':
            metric = getPIT(predictions, sigmas, observations,)
        else:
            raise Exception('unknown probability prediction metric name: '+metric)
        metrics[metricName] = metric
    return metrics

crps = getCRPS(y_pred, y_std, test_y)
print('crps:', crps)
